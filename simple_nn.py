# -*- coding: utf-8 -*-
"""Simple NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZXZiMX98341BCzV8c6K8KO2myKyIKHxa

# **First Simple Neural Network**![Sigmoid.PNG](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMkAAAAtCAYAAAAEE0+RAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAOcSURBVHhe7ZjbceMwDEW3STUTl2LVkXEbGVfhNMEl+JD4AAS9uOTO3DOjD8uSSAH3kE7+GADAJpAEAAVIAoACJAFAAZIAoABJAFCAJAAoQBIAFCAJAAqQBAAFSAKAAiQBQAGSAKAASQBQgCQAKIwvyc9spq+X+YSPMm8zTw/z+g0fW/H7Mo9ptqPVfL4f5vGdzlSe0/s5FdeOxMe8viYz/4SPB6E6TM+8Qty5/4XBJTkY/N1CncQJMpmJlYSClc9VEoHOT/Y5o0pyPdCcZNfE68nQkhxvVrtGuLlQsK0IrCQkUCoou+OQ9CSI3XHsPEVJSHZht2rPTTsyt2C1XsQa0U8SFwS/opaHD7kP1Br4ELAqiEXYNhvhJeLGpODKwUiCIwWYzidCu91C/Mnh53FOkvId1uuiyOki4Xctf4373tbmFXYydxS1itcIM3No43i4BavdItaSLpL4IqehDAKkoeKCkkkRwlLuNO6aG1ZCCSHAFJJSaDkMZyXxz03v87Vcr83C6p6z1iKGe61ZWUNlXglb40TcNUV/uHOj00ESoRFcQ5kVLYZiTpuUoQX0ImyAaczknCoqU4OwACwrfHYkYaxqUj4riPScq122FMqRzfVI7eRxIlwPpb6OTAdJpEbk5+VihtVPbOb+1fAUnCQUtHSuZyRJYUUMNbHvzR3Zs9z99vyugKZ1PyKJRRhngXsP4d1GZjBJ9J0k3l8FY2ErgKtg9bEV6gSmyTTXbLxGkrifKlIgE1aZ6nkelWR9lj/SOUvjLECSswgBKYrHNzQEha5z13NBlCS8iarJ9D7FPJruJErA3Nj07LdfEJLf/+z92VwP1G5jnAjXQ6mvI9PlD3cfgjREYXdIC+2aUDS0uI9dWdWAXqQKsJ171XQvgRw2RRIRqU5xLP/c5fvsuxDQ7P7iegvVVJ/X9jgR7o907tzo9JGEcGGzBQtH3ZgiaKER+XV1aJqvVKUk9JlpupuHGIazkhDhnZMj1mjZZf1HhxfDLxqxNtm/gMs50vso9dPG8fh55uJw58annyQ72A4aRyFWT5zUxU7YmSjJtpoU5Bt2Yk62HQKOyNCSHG7YYE3Y99Pl37FPkjOLUwm3WA20gB1kcEksu4N/0wp4K2PNaa8kVwPNSXZdvH6MLwkAnYEkAChAEgAUIAkACpAEAAVIAoACJAFAAZIAoABJAFCAJAAoQBIAFCAJAAqQBAAFSAKAAiQBQAGSALCJMX8BzwVQJ6tSubIAAAAASUVORK5CYII=)

It takes an input vector, calculates a weighted sum, adds a bias, and then applies the sigmoid activation function to produce a prediction. The result of the prediction is printed to the console.
"""

input_vector = np.array([1.66, 1.56])
weights_1 = np.array([1.45, -0.66])
bias = np.array([0.0])

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def make_prediction(input_vector, weights, bias):
    layer_1 = np.dot(input_vector, weights) + bias
    layer_2 = sigmoid(layer_1)
    return layer_2

prediction = make_prediction(input_vector, weights_1, bias)

print(f"The prediction result is: {prediction}")

import numpy as np

"""The **Sigmoid function** is a special form of the logistic function and is usually denoted by σ(x) or sig(x). It is given by:

#                    **σ(x) = 1/(1+exp(-x))**

**Few properties of Sigmoid function include :**

1.   Domain: (-∞, +∞)
2. Range: (0, +1)
3. σ(0) = 0.5
4. The function is monotonically increasing.
5. The function is continuous everywhere.
6. The function is differentiable everywhere in its domain.
7. Numerically, it is enough to compute this function’s value over a small range of numbers, e.g., [-10, +10]. For values less than -10, the function’s value is almost zero. For values greater than 10, the function’s values are almost one.

# **Why to use Sigmoid function in Neural Network ?**

*   If we use a linear activation function in a neural network, then this model can only learn linearly separable problems.
*   However, with the addition of just one hidden layer and a sigmoid activation function in the hidden layer, the neural network can easily learn a non-linearly separable problem.
*   Using a non-linear function produces non-linear boundaries and hence, the sigmoid function can be used in neural networks for learning complex decision functions.
*   The only non-linear function that can be used as an activation function in a neural network is one which is monotonically increasing.
*   So for example, periodic functions like sin(x) or cos(x) cannot be used as activation functions as they repeat the motion after a period.
*   Also, the activation function should be defined everywhere and should be continuous everywhere in the space of real numbers.
*   The function is also required to be differentiable over the entire space of real numbers.
*   **The fact that the sigmoid function is monotonic, continuous and differentiable everywhere, coupled with the property that its derivative can be expressed in terms of itself, makes it easy to derive the update equations for learning the weights in a neural network when using back propagation algorithm.**

# **Train Your First Neural Network**

*   When training the neural network, you first measure the error and adjust the weights accordingly.
*   The weights will be adjusted using the gradient descent and backpropagation techniques.
*   **Gradient descent** is used to determine the direction and rate at which the parameters should be updated.
*   Before making any changes to the network, you must compute the error.
Calculating the Prediction Error To comprehend the magnitude of the inaccuracy, you must first choose how to measure it.
*   The function used to calculate the error is known as the cost function, or loss function.
*   Here we will be using  the mean squared error (MSE) as the cost function.

You calculate the MSE in two steps:
1.   Compute the difference between the prediction and the target.
2.   Multiply the result by itself.
"""

target =0

mse = np.square(prediction - target)

print(f"Prediction: {prediction}; Error: {mse}")

derivative = 2 * (prediction - target)

print(f"The derivative is {derivative}")

# Updating the weights
weights_1 = weights_1 - derivative

prediction = make_prediction(input_vector, weights_1, bias)

error = (prediction - target) ** 2

print(f"Prediction: {prediction}; Error: {error}")

def sigmoid_deriv(x):
    return sigmoid(x) * (1-sigmoid(x))

derror_dprediction = 2 * (prediction - target)
layer_1 = np.dot(input_vector, weights_1) + bias
dprediction_dlayer1 = sigmoid_deriv(layer_1)
dlayer1_dbias = 1

derror_dbias = (derror_dprediction * dprediction_dlayer1 * dlayer1_dbias)

learning_rate = 0.1

neural_network = NeuralNetwork(learning_rate)

neural_network.predict(input_vector)

"""While **weights** determine the strength of connections between neurons, **biases** provide a critical additional layer of flexibility to neural networks.
 **Biases **are essentially constants associated with each neuron.
 Unlike weights, biases are not connected to specific inputs but are added to the neuron’s output.
"""

class NeuralNetwork:

    def __init__(self, learning_rate):
        self.weights = np.array([np.random.randn(), np.random.randn()]) # Picks a random instance from the dataset
        self.bias = np.random.randn()
        self.learning_rate = learning_rate

#The sigmoid function's ability to transform any real number to one between 0 and 1 is advantageous in data science and many other fields such as:
# In deep learning as a non-linear activation function within neurons in artificial neural networks to allows the network to learn non-linear relationships between the data.
    def _sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

# A derivative is a continuous description of how a function changes with small changes in one or multiple variables.
    def _sigmoid_deriv(self, x):
        return self._sigmoid(x) * (1 - self._sigmoid(x))

    def predict(self, input_vector):
        layer_1 = np.dot(input_vector, self.weights) + self.bias
        layer_2 = self._sigmoid(layer_1)
        prediction = layer_2
        return prediction

    def _compute_gradients(self, input_vector, target):
        layer_1 = np.dot(input_vector, self.weights) + self.bias
        layer_2 = self._sigmoid(layer_1)
        prediction = layer_2

        derror_dprediction = 2 * (prediction - target)
        dprediction_dlayer1 = self._sigmoid_deriv(layer_1)
        dlayer1_dbias = 1
        dlayer1_dweights = (0 * self.weights) + (1 * input_vector)

        derror_dbias = (
            derror_dprediction * dprediction_dlayer1 * dlayer1_dbias
        )
        derror_dweights = (
            derror_dprediction * dprediction_dlayer1 * dlayer1_dweights
        )

        return derror_dbias, derror_dweights

# updates the bias and the weights using _update_parameters(),
    def _update_parameters(self, derror_dbias, derror_dweights):
        self.bias = self.bias - (derror_dbias * self.learning_rate)
        self.weights = self.weights - (
            derror_dweights * self.learning_rate
        )

    def train(self, input_vectors, targets, iterations):
        cumulative_errors = []
        for current_iteration in range(iterations):
            # Pick a data instance at random
            random_data_index = np.random.randint(len(input_vectors))

            input_vector = input_vectors[random_data_index]
            target = targets[random_data_index]

            # Compute the gradients and update the weights
            derror_dbias, derror_dweights = self._compute_gradients(
                input_vector, target
            )

            self._update_parameters(derror_dbias, derror_dweights)

            # Measure the cumulative error for all the instances
            # checks if the current iteration index is a multiple of 100.
            # You do this to observe how the error changes every 100 iterations.
            if current_iteration % 100 == 0:
                cumulative_error = 0
                # Loop through all the instances to measure the error
                for data_instance_index in range(len(input_vectors)):
                    data_point = input_vectors[data_instance_index]
                    target = targets[data_instance_index]

                    prediction = self.predict(data_point)
                    error = np.square(prediction - target)

                    cumulative_error = cumulative_error + error
                cumulative_errors.append(cumulative_error)

        return cumulative_errors

import matplotlib.pyplot as plt
input_vectors = np.array(
      [
        [3, 1.5],
        [2, 1],
        [4, 1.5],
         [3, 4],
        [3.5, 0.5],
        [2, 0.5],
        [5.5, 1],
        [1, 1],
     ]
)

targets = np.array([0, 1, 0, 1, 0, 1, 1, 0])

learning_rate = 0.1

neural_network = NeuralNetwork(learning_rate)

training_error = neural_network.train(input_vectors, targets, 10000)

plt.plot(training_error)
plt.xlabel("Iterations")
plt.ylabel("Error for all training instances")
plt.savefig("cumulative_error.png")